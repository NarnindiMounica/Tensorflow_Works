{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3e084c",
   "metadata": {},
   "source": [
    "### Subword Tokenization with the IMDB Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e98297",
   "metadata": {},
   "source": [
    "##### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbed7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145da04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = tfds.load(\"imdb_reviews\", as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd925a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting reviews and labels\n",
    "\n",
    "train_reviews = imdb['train'].map(lambda review, label: review)\n",
    "test_reviews = imdb['test'].map(lambda review, label: review)\n",
    "train_labels = imdb['train'].map(lambda review, label: label)\n",
    "test_labels = imdb['test'].map(lambda review, label: label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd686cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_reviews.take(2))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a2d02",
   "metadata": {},
   "source": [
    "##### Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb1063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for tokenization and padding\n",
    "\n",
    "vocab_size = 10000\n",
    "max_length = 120\n",
    "padding_type = \"pre\"\n",
    "truncating_type = \"post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bfda008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating vectorization layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
    "\n",
    "#generating vocabulary based on training reviews\n",
    "vectorize_layer.adapt(train_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c521bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_func(sequences):\n",
    "\n",
    "    sequences = sequences.ragged_batch(batch_size=sequences.cardinality())\n",
    "\n",
    "    sequences = sequences.get_single_element()\n",
    "\n",
    "    padded_sequences = tf.keras.utils.pad_sequences(sequences.numpy(), \n",
    "                padding=padding_type,\n",
    "                truncating=truncating_type,\n",
    "                maxlen=max_length)\n",
    "\n",
    "    padded_sequences = tf.data.Dataset.from_tensor_slices(padded_sequences)\n",
    "\n",
    "    return padded_sequences            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6062efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = train_reviews.map(vectorize_layer).apply(padding_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10ef68",
   "metadata": {},
   "source": [
    "The cell above uses a vocab_size of 10000 but you'll find that it's easy to find OOV tokens when decoding using the lookup dictionary it created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be4fadab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    this was an absolutely terrible movie dont be [UNK] in by christopher walken or michael [UNK] both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the [UNK] rebels were making their cases for [UNK] maria [UNK] [UNK] appeared phony and her [UNK] affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher [UNK] good name i could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "#get the vocabulary\n",
    "\n",
    "imdb_vocab_fillword = vectorize_layer.get_vocabulary()\n",
    "\n",
    "#get a sample integer sequence\n",
    "sample_sequence = train_sequences.take(1).get_single_element()\n",
    "\n",
    "#lookup each token in the vocabulary\n",
    "decoded_text = [imdb_vocab_fillword[index] for index in sample_sequence]\n",
    "decoded_text = \" \".join(decoded_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa900ce",
   "metadata": {},
   "source": [
    "* For binary classifiers, this might not have a big impact but you may have other applications that will benefit from avoiding OOV tokens when training the model (e.g. text generation). If you want the tokenizer above to not have OOVs, then you might have to increase the vocabulary size to more than 88k. Right now, it's only at 10k. This can slow down training and bloat the model size. The encoder also won't be robust when used on other datasets which may contain new words, thus resulting in OOVs again.\n",
    "\n",
    "* Subword text encoding gets around this problem by using parts of the word to compose whole words. This makes it more flexible when it encounters uncommon words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf0f00",
   "metadata": {},
   "source": [
    "Firstly, we will compute the subword vocabulary using the compute_word_piece_vocabulary() function.\n",
    "\n",
    "- learn from the train_reviews\n",
    "- set a max vocabulary size of 8k\n",
    "- reserve special tokens similar to the full word vocabulary\n",
    "- save the output to a file in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the subword vocabulary and saving it into a file\n",
    "\n",
    "keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    train_reviews,\n",
    "    vocabulary_size=8000,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\"],\n",
    "    vocabulary_output_file='imdb_vocab_subwords.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c282b26",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "* compute_word_piece_vocabulary requires `tensorflow` and `tensorflow-text` for text processing. Run `pip install tensorflow-text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58abf303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we are using python 3.13 which is not compatible with tensorflow-text>=2.20, we are using pre-downloaded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff27515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the subword tokenizer\n",
    "subword_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=\"./imdb_vocab_subwords.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a07cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the subwords\n",
    "\n",
    "subword_tokenizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d60ed",
   "metadata": {},
   "source": [
    "\n",
    "If we use it on the previous plain text sentence, we will see that it won't have any OOVs even if it has a smaller vocab size (only around 8k compared to 10k above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the size of the subword vocabulary\n",
    "\n",
    "subword_tokenizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e2b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample review\n",
    "sample_review = train_reviews.take(1).get_single_element()\n",
    "\n",
    "# Encode the first plaintext sentence using the subword text encoder\n",
    "tokenized_string = subword_tokenizer.tokenize(sample_review)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "# Decode the sequence\n",
    "original_string = subword_tokenizer.detokenize(tokenized_string)\n",
    "\n",
    "# Print the result\n",
    "print('The original string: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01275e43",
   "metadata": {},
   "source": [
    "Subword encoding can even perform well on words that are not commonly found in movie reviews. First, see the result when using the full-word tokenizer. As expected, it will show many unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9262156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample sentence\n",
    "sample_string = 'TensorFlow, from basics to mastery'\n",
    "\n",
    "# Encode using the plain text tokenizer\n",
    "tokenized_string = vectorize_layer(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "# Decode and print the result\n",
    "decoded_text = [imdb_vocab_fullword[token] for token in tokenized_string]\n",
    "original_string = ' '.join(decoded_text)\n",
    "print ('The original string: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c80a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode using the subword text encoder\n",
    "tokenized_string = subword_tokenizer.tokenize(sample_string)\n",
    "print('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "# Decode and print the results\n",
    "original_string = subword_tokenizer.detokenize(tokenized_string).numpy().decode(\"utf-8\")\n",
    "print('The original string: {}'.format(original_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542450f2",
   "metadata": {},
   "source": [
    "As you may notice, the sentence is correctly decoded. The downside is the token sequence is much longer. Instead of only 5 when using the full-word tokenizer, you ended up with 12 tokens instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6219e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show token to subword mapping:\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, subword_tokenizer.detokenize([ts]).numpy().decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e2dc5",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bda10",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Generate integer sequences using the subword tokenizer\n",
    "train_sequences_subword = train_reviews.map(lambda review: subword_tokenizer.tokenize(review)).apply(padding_func)\n",
    "test_sequences_subword = test_reviews.map(lambda review: subword_tokenizer.tokenize(review)).apply(padding_func)\n",
    "\n",
    "# Combine the integer sequence and labels\n",
    "train_dataset_vectorized = tf.data.Dataset.zip(train_sequences_subword,train_labels)\n",
    "test_dataset_vectorized = tf.data.Dataset.zip(test_sequences_subword,test_labels)\n",
    "\n",
    "# Optimize the datasets for training\n",
    "train_dataset_final = (train_dataset_vectorized\n",
    "                       .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "                       .cache()\n",
    "                       .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                       .batch(BATCH_SIZE)\n",
    "                       )\n",
    "\n",
    "test_dataset_final = (test_dataset_vectorized\n",
    "                      .cache()\n",
    "                      .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                      .batch(BATCH_SIZE)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2509020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensionality of the embedding\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(MAX_LENGTH,)),\n",
    "    tf.keras.layers.Embedding(subword_tokenizer.vocabulary_size(), EMBEDDING_DIM),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128025c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Set the training parameters\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Start training\n",
    "history = model.fit(train_dataset_final, epochs=num_epochs, validation_data=test_dataset_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393e3a4",
   "metadata": {},
   "source": [
    "##### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e5e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "  '''Plots the training and validation loss and accuracy from a history object'''\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  epochs = range(len(acc))\n",
    "\n",
    "  fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "  ax[0].plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "  ax[0].plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "  ax[0].set_title('Training and validation accuracy')\n",
    "  ax[0].set_xlabel('epochs')\n",
    "  ax[0].set_ylabel('accuracy')\n",
    "  ax[0].legend()\n",
    "\n",
    "  ax[1].plot(epochs, loss, 'bo', label='Training Loss')\n",
    "  ax[1].plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "  ax[1].set_title('Training and validation loss')\n",
    "  ax[1].set_xlabel('epochs')\n",
    "  ax[1].set_ylabel('loss')\n",
    "  ax[1].legend()\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "plot_loss_acc(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
