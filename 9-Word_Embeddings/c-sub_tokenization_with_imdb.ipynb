{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3e084c",
   "metadata": {},
   "source": [
    "### Subword Tokenization with the IMDB Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e98297",
   "metadata": {},
   "source": [
    "##### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbed7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145da04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = tfds.load(\"imdb_reviews\", as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd925a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting reviews and labels\n",
    "\n",
    "train_reviews = imdb['train'].map(lambda review, label: review)\n",
    "test_reviews = imdb['test'].map(lambda review, label: review)\n",
    "train_labels = imdb['train'].map(lambda review, label: label)\n",
    "test_labels = imdb['test'].map(lambda review, label: label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd686cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_reviews.take(2))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a2d02",
   "metadata": {},
   "source": [
    "##### Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb1063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for tokenization and padding\n",
    "\n",
    "vocab_size = 10000\n",
    "max_length = 120\n",
    "padding_type = \"pre\"\n",
    "truncating_type = \"post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bfda008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating vectorization layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
    "\n",
    "#generating vocabulary based on training reviews\n",
    "vectorize_layer.adapt(train_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c521bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_func(sequences):\n",
    "\n",
    "    sequences = sequences.ragged_batch(batch_size=sequences.cardinality())\n",
    "\n",
    "    sequences = sequences.get_single_element()\n",
    "\n",
    "    padded_sequences = tf.keras.utils.pad_sequences(sequences.numpy(), \n",
    "                padding=padding_type,\n",
    "                truncating=truncating_type,\n",
    "                maxlen=max_length)\n",
    "\n",
    "    padded_sequences = tf.data.Dataset.from_tensor_slices(padded_sequences)\n",
    "\n",
    "    return padded_sequences            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6062efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = train_reviews.map(vectorize_layer).apply(padding_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10ef68",
   "metadata": {},
   "source": [
    "The cell above uses a vocab_size of 10000 but you'll find that it's easy to find OOV tokens when decoding using the lookup dictionary it created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4fadab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    this was an absolutely terrible movie dont be [UNK] in by christopher walken or michael [UNK] both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the [UNK] rebels were making their cases for [UNK] maria [UNK] [UNK] appeared phony and her [UNK] affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher [UNK] good name i could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "#get the vocabulary\n",
    "\n",
    "imdb_vocab_fillword = vectorize_layer.get_vocabulary()\n",
    "\n",
    "#get a sample integer sequence\n",
    "sample_sequence = train_sequences.take(1).get_single_element()\n",
    "\n",
    "#lookup each token in the vocabulary\n",
    "decoded_text = [imdb_vocab_fillword[index] for index in sample_sequence]\n",
    "decoded_text = \" \".join(decoded_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa900ce",
   "metadata": {},
   "source": [
    "* For binary classifiers, this might not have a big impact but you may have other applications that will benefit from avoiding OOV tokens when training the model (e.g. text generation). If you want the tokenizer above to not have OOVs, then you might have to increase the vocabulary size to more than 88k. Right now, it's only at 10k. This can slow down training and bloat the model size. The encoder also won't be robust when used on other datasets which may contain new words, thus resulting in OOVs again.\n",
    "\n",
    "* Subword text encoding gets around this problem by using parts of the word to compose whole words. This makes it more flexible when it encounters uncommon words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf0f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff2261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
